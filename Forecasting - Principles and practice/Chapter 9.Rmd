---
title: "Chapter 9"
author: "Yifei Liu"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=F, warning=F}
library(forecast)
library(seasonal)
library(urca)
library(readxl)
library(rdatamarket)
library(fma)
library(astsa)
library(tsibble)
library(fpp2)
library(tsibbledata) # devtools::install_github("tidyverts/tsibbledata")
library(broom)
library(tidyquant)
library(tidyverse)
library(sweep)
library(lubridate)
detach("package:dplyr", unload=TRUE)
library(dplyr)
library(tseries)
library(scales)
library(GGally)
library(fpp2)
library(fpp2)
library(timetk)
library(gridExtra)
library(ggthemes)
theme_set(theme_minimal())
```

The time series class I took, no matter how accurate they predict, such as the the ETS model I have for  the US net electricity generate, even thoug the predictive value accurate predicted the number for freaking 15 yeras!!!. I still find this two model I previous used, ETS and ARIMA model have lack some important component. 

These two model incorporate the information from the past observations of a series, but not for the inclusion of other information that may also be relevant, or can think about electricity usage from a long term, when new energy usage appear such as air conditioner, the movie ticket sales when deal with emerging of streaming service, the revenue of traditional cable tv company face netflix and chill :). On the other hand regression model  abble to inclusion of a lot of relevant information from predictor variables, but do not allow for the subtle time series dynamic that can handled with ARIMA models. 

simple linear regression model: $y_t = \beta_0 + \beta_1x_{1,t} + ... + \beta_kx_{k,t} + \epsilon_t$

where $y_t$ is a linear function of the k predictor variables and $\epsilon_t$ is usually assumed to be an uncorrelated error time. 

In this chapter, we will allow the errors from a regression model to contain autocorrelation. if $\eta_t$ follow an ARIMA(1,1,1) model, we can write

$y_t = \beta_0 + \beta_1x_{1,t} + ... + \beta_kx_{k,t} + \eta_t$
$(1-\phi_1B)(1-B)\eta_t = (1+\theta_1B)\epsilon_t$

In summary, we incorporate time series model into regression error term. Which mean the error term from a time series regression model is not white noise and only ARIMA model errors are assumed to be white noise. 
9.1 Estimation

Potential problem rising when use minimise the sume of squared $\eta_t$ values. (see problems on the book)

Instead we can minimising the sum of squared $\epsilon_t$ values to avouds these problems. (PS: if we need to fit time series regression model we may face multiple issues such as are we achieve global mim $\epsilon_t$ value of just a local one), alternatively, maximum likeihood estiamtion can be used. This will give similar estiamte of the coefficients. 

We need to make sure the predicators and $y_t$ to be stationary, this step is essentail to make sure the coefficient will be consistent estiamted. One exception to this is the caese of non-stationary $y_t$ and the predictros that is stationary, then the estiamted coefficient will be consistent. 


## 9.2 Regression with ARIMA errors in R


$fit = Arima(y, xreg = x, order = c(1,1,0))$ will fit the model $y_t' = \beta_1x_t' + \eta_t'$, where $\eta_t' = \phi_1\eta_{t-1}' + \epsilon_t$ is an AR(1) error. this is equivalent to the model $y_t = \beta_0 + \beta_1x_t + \eta_t$


### Example: US personal consumption and Income


```{r}
autoplot(uschange[, 1:2], facets = TRUE) +
  labs(title = "Quarterly changes in US consumption and personal income",
       x = "",  y = "")

```

```{r}
(fit <- auto.arima(uschange[, "Consumption"], xreg = uschange[, "Income"]))
 
 
```
The data clearly already stationary (since we considering the percentage chagne rather than raw expenditure and income, which mean we already take difference), the fitted model is 

$y_t = 0.599 + 0.203x_t + \eta_t$
$\eta_t = 0.692\eta_{t-1} + \epsilon_t -0.5758\epsilon_{t-1} +0.1984\epsilon_{t-2}$
$e_t \sim NID(0, 0.322$, NID mean Gaussian and independently distributed

```{r}
cbind("regression error" = residuals(fit, type = "regression"),
      "ARIMA error" = residuals(fit, type = "innovation")) %>%
  autoplot()


```

It is the ARIMA erros that should resemble a white noise series. 

```{r}
checkresiduals(fit)
```


## 9.3 Forecasting


To Forecast using a regression model with ARIMA errors, we need to forecast the regression part of the modela and ARIMA part of the model, and combine the results. As with ordinary regression models in order to obtain forecasts we first need to forecast the preictors. When the predictors are known into the future (e.g.calender-related variables such as time, day of week etc. ) this could be straightforward, but when the predictors are themselves unknown, we must either model them separately, or use assumed future aleus for each predictors. 

### Example: US personal consumption and Income

```{r}
fcast <- forecast(fit, xreg = rep(mean(uschange[, 2]), 8))

auto_fit <- auto.arima(uschange[, "Consumption"], seasonal = F, allowmean = T)
auto_for <- forecast(auto_fit, h = 8)

x <- autoplot(fcast) +
  labs(x = "", y = "Percentage Change")

y <- autoplot(auto_for) +
  labs(x = "", y = "Percentage Change")

grid.arrange(x,y, ncol = 1)

```

We can see the interval for these two model, the lm + ARIMA moded have a narrow forecast interval than simpel ARIMA model, because we are now able to explain some of the variation in the data using the income predictor 

QUESTION: in terms of prediction interval, I still think we need to back test the interval to see does this narrow or wider interval actuall mean anthing. Null hypothesis is, **shorter interal is just accurate as wider interval**, we can use _tscv_ to test it maybe. 

### Example: Forecasting electricity demand

```{r}
autoplot(elecdaily[, c(1,3)], facets = T, colour = T) +
  scale_color_manual(values = c("red", "blue"))+
  theme(legend.position = "none") +
  labs(y = "")

```

In this case, we fit a quadratic regression model with ARMA erro using auto.arima function

```{r}
xreg <- cbind(MaxTemp = elecdaily[, "Temperature"],
              MaxTempSq = elecdaily[, "Temperature"]^2,
              Workday = elecdaily[, "WorkDay"])
(fit <- auto.arima(elecdaily[, "Demand"], xreg = xreg))

checkresiduals(fit)

```

The model have some significant autocorrelation in the residuals, which means the predication interval may not provide accuracy coverage. Also, the histogram of the resduals also show positive outliner, which will also effect the coverage of the prediction intervals. 

Using the estiamte model we forecast the next 14 days ahead (a non-week-day being a public holiday for new years day) In this case, we could obtain weather forecast from the weather bureau for the next 14 days. 

```{r}
fcast <- forecast(fit, xreg = cbind(MaxTemp = rep(26, 14),
                                    MaxTempSq = rep(26^2, 14),
                                    Workday = c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))

autoplot(fcast) +
  labs(y = "Electricity demand (GW)")

```

The slow down in electricity demand at the end of 2014 has caused the forecasts for the next two weeks to show similarly low demand values. 


## 9.4 Stochastic and deterministic trends

Two different way to modelling a _linear trend_.

A deterministic trend is obtained using the regression model. 

$y_t = \beta_0 + \beta_1t + \eta_t$ where $\eta_t$ is an ARMA process

A stochastic trend is obtained using the model

$y_t = \beta_0 + \beta_1t +\eta_t$, where $\eta_t$ is an ARIMA process with d = 1. 
In the latter case, we can difference both sides so that $y_t' = \beta_1 = \eta_t'$, where $\eta_t'$ is an ARMA process. In other words $y_t = y_{t-1} + \beta_1 + \eta_t'$. Where is similar to a random walk with drift, but here the error term is an ARMA process rather than simply white noise. 

Althouth these models appear quite similar, their forecasting characterisitics are quite different. 


### Example: International visitors to Australia

```{r}

autoplot(austa) +
  labs(x = "", y = "",
       title = "Total annual international visitors to Australia")

austa %>%
  tk_tbl(rename_index = "Date") %>%
  ggplot(aes(Date, value)) +
  geom_line() +
  geom_smooth(method = "lm", se = F) +
  labs(x = "", y = "",
       title = "Total annual international visitors to Australia")

```

This graphic show the international visitors to Australia from 1980 to 2015. Now we can use both model to model this _linear trend_. After use *lm* model we can see this data basically is linear trend. 

Deterministic trend model:

```{r}
trend <- seq_along(austa)
(fit1 <- auto.arima(austa, d = 0, xreg = trend))

```

this mdoel can be written as:

$y_t = 0.4156 + 0.1710t = \eta_t$

$\eta_t = 1.1127\eta_{t-1} - 0.3805\eta_{t-2} + \epsilon_t$ 

$\epsilon_t \sim NID(0, 0.02979)$

The estiamte growth in visitor number is 0.17 million people per year. 

Alternative, use stochastic trend model to estiamte this one. 

```{r}
(fit2 <- auto.arima(austa, d = 1))

```

This model can be written as: $y_t - y_{t-1} = 0.1735 + \eta_t$

$y_t = y_0 + 0.1735 t + \eta_t$

$\eta_t = \eta_{t-1} + 0.3006 \epsilon_{t-1} + \epsilon_t$

$\epsilon_t \sim NID(0, 0.03376)$

We can see now both model estimate number around 0.17 million people increase per year. But the prediction intervals are not. In particular, stochastic trend have much more wider prediction intervals buecause of **error terms are non-stationary**.

```{r}
fc1 <- forecast(fit1, 
                xreg = cbind(length(austa) + 1:10))

fc2 <- forecast(fit2, h = 10)


autoplot(austa) +
  autolayer(fc2, series="Stochastic trend") +
  autolayer(fc1, series="Deterministic trend") +
  labs(y = "Visiotors to Australia (millions)",
       title = "Forecast from two linear trend models",
       x = "",
       color = "Forecast Model") +
  theme(legend.position = "bottom")


```

The implication assumption with deterministic trends that the slope of the trend is not going to change over time. On the other hand, stochastic trend can change, and the estimated growth is only assumed to be the average grwoth over the historical period, not necessarily the rate of grwoth taht will be observed into the future. 

## 9.5 Dynamic harmonic regression

*when there are long seasonal periods, a dynamic regression with Fourier terms is often bettern than other models we have considered in this book*

Plenty of reasons why previous model such as seasonal ARIMA, ETS model have a hard time to predict longer periods. For instance, the Arima and auto.arma function, m is up to 350, but in practice, we often run out of memory wheeve the season period is more than 200. 

For such time series data, we perfer a harmonic regression approach where the seasonal pattern is modellied using Fourier terms with short-term time series dynamci handled by an ARMA error

The only real disadvantage compare to ARIMA model is that the seasonality is assumed to be fixed - the seasonal pattern is not allowed to chnage over time. But in practice, seasonality is usually remarkably constant. 

### Example: Australia eating out expenditure.


```{r}
cafe04 <- window(auscafe, start = 2004)

plots <- list()

for (i in seq(6)) {
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
    seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit,
      xreg=fourier(cafe04, K=i, h=24))) +
    labs(x = paste("K=",i,"   AICC=",round(fit[["aicc"]],2)),
         y = "")
}

grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)

```



## 9.6 Lagged predictors

Sometimes, the impact of a predictor which is included in a regression model will not be simple and immediate. 

We needed to allow for lagged effects of the predictors. 

$y_t = \beta_0 + \gamma_0x_t + \gamma_1x_{t-1} + ... + \gamma_kx_{t-k} + \eta_t$,  where $\eta_t$ is an ARIMA prcocess. the value of k can be selected using the AICs, along with the values of p and q for the ARIMA eoor. 

QUESTIONS: then what about d?

### Example: TV advertising and insurance quotations

```{r}
autoplot(insurance, facets = T) +
  labs(x = "",
       y = "",
       title = "Insurance advertising and quotations")

```

Let's assume the advertising expenditure for up to four months. which is the model include the ad spend this month and up to previous three months before that. 


```{r}
# lagged periodtors, test 0, 1, 2 or 3 lags

Advert <- cbind(
    AdLag0 = insurance[,"TV.advert"],
    AdLag1 = stats::lag(insurance[,"TV.advert"],-1),
    AdLag2 = stats::lag(insurance[,"TV.advert"],-2),
    AdLag3 = stats::lag(insurance[,"TV.advert"],-3)) %>%
  head(NROW(insurance))


# restrict data so models use same fitting period

fit1 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1],
  stationary=TRUE)
fit2 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:2],
  stationary=TRUE)
fit3 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:3],
  stationary=TRUE)
fit4 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:4],
  stationary=TRUE)


```

Next we choose the optimal lag lneght for advertising based on the AICc.

```{r}

# c(fit1[["aicc"]],fit2[["aicc"]],fit3[["aicc"]],fit4[["aicc"]])

# the original way to display AIC values, I think map or lapply are a better way to display and do the loop things. If you think a lot of your work follow the loop, you may consider map and apply as a clear and easy alternatives. 

map_dbl(list(fit1, fit2, fit3, fit4), ~ .x[["aicc"]]) %>%
  set_names(paste0("lag", 1:4))

```

The best model (with the smallest AICc valeus) has two lagged preidctos. That is it included advertising only in the current month and previous months. So we now re-estomate that model, but using all the avaialbel data. 

QUESTIONS: I still question and consistance of this AICc valeus, if I'm not only use half the data as traning, instead I use 80% of data will AIC yield the similary result?


```{r}
# restrict data so models use same fitting period

fit1 <- auto.arima(insurance[4:32,1], xreg=Advert[4:32,1],
  stationary=TRUE)
fit2 <- auto.arima(insurance[4:32,1], xreg=Advert[4:32,1:2],
  stationary=TRUE)
fit3 <- auto.arima(insurance[4:32,1], xreg=Advert[4:32,1:3],
  stationary=TRUE)
fit4 <- auto.arima(insurance[4:32,1], xreg=Advert[4:32,1:4],
  stationary=TRUE)

map_dbl(list(fit1, fit2, fit3, fit4), ~ .x[["aicc"]]) %>%
  set_names(paste0("lag", 1:4))


```

Use 80% of data yield similary result. Use all available data to fit the model

```{r}
(fit <- auto.arima(insurance[,1], xreg=Advert[,1:2],
  stationary=TRUE))


```

The chosen model has AR(3) errors. The model can be written as

$y_t = 2.0393 + 1.2564y_{t-1} + 0.1625y_{t-2} + \eta_t$

$\eta_t = 1.4117\eta_{t-1} -0.9317\eta_{t-2} 0.3591\eta_{t-1}  + \epsilon_t$

$\epsilon \sim NID(0, 0.2165)$


We can calculate forecast using the mdoel if we assume future values for the advertising variables. If we just assume the future monthly ad expenditure remain 8 units, we get the forecast in 


```{r}

ad_spend <- insurance[, 2] %>% as.numeric()

selec_ad_spend <- sample(ad_spend, 24, replace = T)

fc8 <- forecast(fit, h = 24,
                xreg = cbind(AdLag0 = rep(8, 20),
                             AdLag1 = c(Advert[40, 1], rep(8, 19))))

autoplot(fc8) +
  labs(x = "", y = "Quotes",
       title = "Forecast Quotes with future advertising set to 8",
       subtitle = "Forecast horizon 2 years")

# random select ad spending from previous data

fc_rand <- forecast(fit, h = 24,
                xreg = cbind(AdLag0 = selec_ad_spend,
                             AdLag1 = c(Advert[40, 1], selec_ad_spend[-24])))

autoplot(fc_rand) +
  labs(x = "", y = "Quotes",
       title = "Forecast Quotes",
       subtitle = "Ad spend random select from previous data")


```


## 9.7 Exercises. 

1. 

a. Plot the data using autoplot. Why is it useful to set facets=TRUE?

```{r}
autoplot(advert, facets = T)


```

b. Fit a standard regression model $y_t = a + bx_t + \eta_t$ where $y_t$denotes advertising using the tslm() function.

```{r}
(fit_tslm <- tslm(sales ~ advert, data = advert))


```

c. Show that the residuals have significant autocorrelation.

```{r}
checkresiduals(fit_tslm)


```

The ACF plot show the residuals have significant autocorrelation at lag 1 and lag 2. 

d. What difference does it make you use the Arima function instead:

```{r}
fit_arima <- Arima(advert[, "sales"], order = c(0,0,0))

checkresiduals(fit_arima)

fit_arima_xreg <- Arima(advert[, "sales"],xreg = advert[, "advert"], order = c(0,0,0))

checkresiduals(fit_arima_xreg)

fit_res <- tibble(
  arima_res = fit_arima$residuals,
  xreg_res = fit_arima_xreg$residuals,
  tslm_res = fit_tslm$residuals
)

fit_res %>%
  mutate(xreg_dif = xreg_res - tslm_res,
         arima_dif = arima_res - tslm_res) %>%
  select(xreg_dif, arima_dif)

```

we can see in the result, residuals difference between arima with xreg argument identical to tslm function. but only arima model is is significant different than tslm model. 

e. Refit the model using auto.arima(). How much difference does the error model make to the estimated parameters? What ARIMA model for the errors is selected?

```{r}
(fit_auto <- auto.arima(advert[, "sales"], xreg = advert[, "advert"]))
fit_arima_xreg

```
arima xreg model is, $y_t =  78.7343 + 0.5343x_t + \eta_t$, where $\eta_t$ is autocorrelated error term.

auto xreg model is $y'_t = 0.5063x_t' + \eta_t'$, where $\eta_t' = \epsilon_t$. 

f. Check the residuals of the fitted model.

```{r}
checkresiduals(fit_auto)

```

g. Assuming the advertising budget for the next six months is exactly 10 units per month, produce and plot sales forecasts with prediction intervals for the next six months.

```{r}
auto_for <- forecast(fit_auto, xreg = rep(10, 6))

autoplot(auto_for) +
  labs(subtitle = "Assume ad spending for the next 6 months is flat 10 units")

```

2. 

a. Fit a piecewise linear trend model to the Lake Huron data with a knot at 1920 and an ARMA error structure.

```{r}
huron_time <- ts(pmax(0, time(huron)- 1920), start = time(huron)[1])


huron_xreg <- cbind(hur_time = time(huron),
                    t_pw = huron_time)

(huron_auto <- auto.arima(huron, xreg = huron_xreg))



```
a piecewise linear trend model with ARIMA(2,0,0) regression error term, and a $\epsilon_t \sim NID(0, 0.459)$

b. Forecast the level for the next 30 years.

```{r}
huron_xreg <- cbind(hur_time = end(huron)[1] + 1:30,
                    t_pw = 52 + 1:30)

huron_for <- forecast(huron_auto, xreg = huron_xreg, h = 30)

autoplot(huron_for)

checkresiduals(huron_for)

```

Use the piecewise model we forecast the level of lake will gradually increase of the next 30 years. 

The residuals from the model seem to be white noise. 

3. 

a. Use the data to calculate the average cost of a night’s accommodation in Victoria each month.

```{r}
autoplot(motel, facets = T)

avg_hotel <- motel %>%
  tk_tbl() %>%
  mutate(avg = 1000 * Takings / Roomnights) %>%
  select(index, avg) 

avg_hotel %>%
  ggplot(aes(index, avg)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar_format())

avg_ts <- avg_hotel %>%
  tk_ts(select = avg, start = c(1980, 1), frequency = 12)

```

b. Use cpimel to estimate the monthly CPI.

```{r}
cpi_auto <- auto.arima(motel[, "CPI"])

autoplot(motel[, "CPI"]) +
  autolayer(cpi_auto$fitted)

```

c. Produce time series plots of both variables and explain why logarithms of both variables need to be taken before fitting any models.

```{r}
autoplot(avg_ts)
autoplot(cpi_auto$fitted)

```

logarithms can make the variations stable cross time. remove heteroskedasticity inside time sieries. 

d. Fit an appropriate regression model with ARIMA errors. Explain your reasoning in arriving at the final model.

```{r}
avg_cost <- auto.arima(avg_ts, xreg = (motel[, "CPI"]))

checkresiduals(avg_cost)
ur.df(avg_cost$residuals) %>%
  summary()
```

The residuals from regression with ARIMA model look like white noise and also pass ur.df test. 

e. Forecast the average price per room for the next twelve months using your fitted model. (Hint: You will need to produce forecasts of the CPI figures first.)

```{r}
cpi_for <- forecast(cpi_auto, h = 12)

avg_cost_for <- forecast(avg_cost, xreg = cpi_for$mean, h = 12)

autoplot(avg_cost_for)

```

4. 

Using tslm(), fit a harmonic regression with a piecewise linear time trend to the full gasoline series. Select the position of the knots in the trend and the appropriate number of Fourier terms to include by minimising the AICc or CV value.

```{r}
autoplot(gasoline)

```

```{r}
t_knot <- time(gasoline)
t_knot_1 <- 2007.5
t_know_2 <- 2013

t_pw_1 <- ts(pmax(0, t_knot - t_knot_1), start = t_knot[1], frequency = frequency(gasoline))
t_pw_2 <- ts(pmax(0, t_knot - t_know_2), start = t_knot[1], frequency = frequency(gasoline))


#reason choose 25 is the max number of repetion should be less than half of the frequency which in this case is 52
aicc_val <- NULL

for(num in 1:26) {
  gas_tslm <- tslm(gasoline ~ trend + t_pw_1 + t_pw_2 + fourier(gasoline, K = num))
  aicc_val[num] <- CV(gas_tslm)["AICc"]
}

which.min(aicc_val)

# 12 fourier paris have the lowest aicc values. need to review 5.5 selecting predictors. 

tibble(index = 1:26,
       aicc = aicc_val) %>%
  ggplot(aes(index, aicc)) +
  geom_point() +
  geom_smooth() +
  labs(x = "the numer of fourier sin and cos pairs",
       y = "AICc values",
       title = "Relationship between num of fourier and AICc value")

gasoline_tslm <- tslm(gasoline ~ trend + t_pw_1 + t_pw_2 + fourier(gasoline, K = 12))

autoplot(gasoline, colour = "gray50") +
  autolayer(gasoline_tslm$fitted.values, color = "red")

# let's compare CV and AICc value, see do they yield conflict result

CV_val <- list()

for(num in 1:26) {
  gas_tslm <- tslm(gasoline ~ trend + t_pw_1 + t_pw_2 + fourier(gasoline, K = num))
  CV_val[[num]] <- CV(gas_tslm)
}

CV_list <- map(CV_val, ~.x %>% as.list())

aicc_val <- CV_list %>%
  map_dbl(~.x$AICc)

tibble(index = 1:26,
       aicc = aicc_val) %>%
  ggplot(aes(index, aicc)) +
  geom_point() +
  geom_smooth() +
  labs(x = "the numer of fourier sin and cos pairs",
       y = "AICc values",
       title = "Relationship between num of fourier and AICc value")

CV_cv_val <- CV_list %>%
  map_dbl(~.x$CV)

tibble(index = 1:26,
       CV = CV_cv_val) %>%
  ggplot(aes(index, CV)) +
  geom_point() +
  geom_smooth() +
  labs(x = "the numer of fourier sin and cos pairs",
       y = "AICc values",
       title = "Relationship between num of fourier and AICc value")


```

we can see as the number of K incrase, the first couple of incrase have more significant impact on decrease AICc value the the rest. 

The fitted chart, as we expected able to capture the general trend in the gasoline. Since we manual input piecewise value. 

After examin AICc and CV value, they yield similary result. 


b. Now refit the model using auto.arima() to allow for correlated errors, keeping the same predictor variables as you used with tslm().

```{r}
gasoline_auto <- Arima(gasoline,
                            xreg = cbind(time = t_knot,
                                         t_pw_1 = t_pw_1,
                                         t_pw_2 = t_pw_2,
                                         fourier = fourier(gasoline, K = 12)),
                       order = c(4, 0, 2), seasonal = c(1, 0, 0))


```

c. Check the residuals of the final model using the checkresiduals() function. Do they look sufficiently like white noise to continue? If not, try modifying your model, or removing the first few years of data.

```{r}
checkresiduals(gasoline_auto)

```


d. Once you have a model with white noise residuals, produce forecasts for the next year.

```{r}
autoplot(gasoline) +
  autolayer(forecast(gasoline_auto, h = 52))

```


5. I just ignore this one. 

6. 

```{r}
retail_data <- readxl::read_excel("/Users/yifeiliu/Documents/R/data/book_exercise/forecasting/retail.xlsx", skip = 1)

myts <- ts(retail_data[, "A3349873A"],
           frequency = 12, start = c(1982, 4))

autoplot(myts)

```

a. Develop an appropriate dynamic regression model with Fourier terms for the seasonality. Use the AIC to select the number of Fourier terms to include in the model. 


```{r}
retail_lambda <- BoxCox.lambda(myts)

autoplot(myts)
# we can see there are couple 

t_knot <- time(myts)
t_knot_1 <- 2002
t_knot_2 <- 2010

t_pw_1 <- ts(pmax(0, t_knot - t_knot_1), start = t_knot[1], frequency = frequency(myts))
t_pw_2 <- ts(pmax(0, t_knot - t_knot_2), start = t_knot[1], frequency = frequency(myts))

#reason choose 5 is the max number of repetion should be less than half of the frequency which in this case is 12
aicc_val <- NULL

for(num in 1:5) {
  retail_tslm <- tslm(myts ~ trend + t_pw_1 + t_pw_2 + fourier(myts, K = num), lambda = retail_lambda)
  aicc_val[num] <- CV(retail_tslm)["AICc"]
}

which.min(aicc_val)

# make harmonic regression model using the lowest AICc number of pairs, in this case K = 5

retail_arima <- auto.arima(myts, xreg = cbind(time = t_knot,
                                         t_pw_1 = t_pw_1,
                                         t_pw_2 = t_pw_2,
                                         fourier = fourier(myts, K = 5)),
                            lambda = retail_lambda)


autoplot(myts) +
  autolayer(retail_arima$fitted)

```

b. Check the residuals of the fitted model. Does the residual series look like white noise?

```{r}
checkresiduals(retail_arima)
```
The residuals look like white noise. 


c. Compare the forecasts with those you obtained earlier using alternative models.

```{r}
retail_train <- window(myts, start = c(1982, 4), end = c(2012, 12))
retail_test <- window(myts, start = c(2013,1))

retail_fit1 <- rwf(retail_train, h = 12)
retail_fit2 <- snaive(retail_train, h = 12)
retail_fit3 <- auto.arima(retail_train, lambda = retail_lambda) %>%
  forecast(h = 12)
retail_fit4 <- ets(retail_train) %>%
  forecast(h = 12)

t_knot_train <- time(retail_train)
t_knot_1_train <- 2002
t_knot_2_train <- 2010
t_pw_1_train <- ts(pmax(0, t_knot_train - t_knot_1_train), start = t_knot_train[1], frequency = frequency(retail_train))
t_pw_2_train <- ts(pmax(0, t_knot_train - t_knot_2_train), start = t_knot_train[1], frequency = frequency(retail_train))

xreg_train <- cbind(
  Fourier = fourier(retail_train, K = 5),
  time = t_knot_train,
  t_pw_1 = t_pw_1_train,
  t_pw_2 = t_pw_2_train
)

t_knot_test <- time(retail_test)
t_knot_1_test <- 2002
t_knot_2_test <- 2010
t_pw_1_test <- ts(pmax(0, t_knot_test - t_knot_1_test), start = t_knot_test[1], frequency = frequency(retail_test))
t_pw_2_test <- ts(pmax(0, t_knot_test - t_knot_2_test), start = t_knot_test[1], frequency = frequency(retail_test))

xreg_test <- cbind(
  Fourier = fourier(retail_train, K = 5, h = 12),
  time = time(retail_test),
  t_pw_1 = t_pw_1_test,
  t_pw_2 = t_pw_2_test
)

retail_fit5 <- auto.arima(retail_train,
                          xreg = xreg_train,
                          lambda = retail_lambda) %>%
  forecast(h = 12, xreg = xreg_test)
  


x <- map2(list(retail_fit1, retail_fit2, retail_fit3, retail_fit4, retail_fit5), list(retail_test, retail_test, retail_test,retail_test, retail_test), accuracy)

x <- x %>%
  set_names(c("rwf", "snaive", "auto.arima", "ets", "tslm_arima")) 

x %>%
  enframe("username", "x") 

retail_accuracy <- map_dfr(x, as.data.frame) %>%
  add_column(type = rep(c("train", "test"), 5),
             method = rep(c("rwf", "snaive", "auto.arima", "ets", "tslm_arima"), each = 2))

retail_accuracy %>%
  ggplot(aes(method, RMSE, fill = type)) +
  geom_col(position = "dodge")

retail_accuracy %>%
  arrange(RMSE)

```
we can say the tslm with arima error term model have the lowest RMSE value, so this model is the 

```{r}
retail_best <- auto.arima(retail_train, xreg = xreg_train, lambda = retail_lambda)

retail_best
```
Here is our best model used for forecast retail data. 

```{r}
autoplot(retail_train) +
  autolayer(retail_test, color = "red") +
  autolayer(retail_fit5, color = "purple", alpha = 0.5)


```



